{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c54031-b5bd-4316-9303-7af0d3443ca3",
   "metadata": {
    "name": "STEP_1_CREATE_DEMO_DATA",
    "collapsed": false
   },
   "source": "# 1 - Create Demo Data"
  },
  {
   "cell_type": "code",
   "id": "5f05c3de-e7b5-4a13-8cb4-21c86099e352",
   "metadata": {
    "language": "sql",
    "name": "CREATE_DATA",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Create fake data\ncreate or replace table ECOMMERCE_CUSTOMERS (email string, gender string, MEMBERSHIP_STATUS string,  MEMBERSHIP_LENGTH double, AVG_SESSION_LENGTH double, TIME_ON_APP double, TIME_ON_WEBSITE double, YEARLY_SPENT double) as \nwith profiles as (\nselect concat(lower(randstr(10, random())), '@', lower(randstr(5, random())), '.com') as EMAIL,\n        case when uniform(1,10,random())<=7 then 'MALE'\n            when uniform(1,10,random())<=10 then 'FEMALE'\n        end as GENDER,\n        uniform(100,75000,random()) / 100 as YEARLY_SPENT,\n        case when YEARLY_SPENT < 150 then 'BASIC'\n            when YEARLY_SPENT < 250 then 'BRONZE'\n            when YEARLY_SPENT < 350 then 'SILVER'\n            when YEARLY_SPENT < 550 then 'GOLD'\n            when YEARLY_SPENT < 650 then 'PLATIN'\n            when YEARLY_SPENT < 720 then 'DIAMOND'\n        end as MEMBERSHIP_STATUS,\n        case when YEARLY_SPENT < 150 then null\n            when YEARLY_SPENT < 250 then uniform(50,150,random()) / 100\n            when YEARLY_SPENT < 350 then uniform(250,350,random()) / 100\n            when YEARLY_SPENT < 550 then uniform(300,550,random()) / 100\n            when YEARLY_SPENT < 650 then uniform(500,750,random()) / 100\n            when YEARLY_SPENT < 720 then uniform(700,1000,random()) / 100\n        end as MEMBERSHIP_LENGTH,\n        case when YEARLY_SPENT < 120 then null\n            when YEARLY_SPENT < 150 then uniform(500,750,random()) / 100\n            when YEARLY_SPENT < 250 then uniform(700,1000,random()) / 100\n            when YEARLY_SPENT < 350 then uniform(900,2000,random()) / 100\n            when YEARLY_SPENT < 550 then uniform(1900,2700,random()) / 100\n            when YEARLY_SPENT < 650 then uniform(2500,3200,random()) / 100\n            when YEARLY_SPENT < 1000 then uniform(3000,4000,random()) / 100\n        end as AVG_SESSION_LENGTH,\n        case when YEARLY_SPENT < 150 then uniform(5000,7500,random()) / 100\n            when YEARLY_SPENT < 250 then uniform(7300,10000,random()) / 100\n            when YEARLY_SPENT < 350 then uniform(9500,20000,random()) / 100\n            when YEARLY_SPENT < 370 then null\n            when YEARLY_SPENT < 550 then uniform(19000,27000,random()) / 100\n            when YEARLY_SPENT < 650 then uniform(25000,32000,random()) / 100\n            when YEARLY_SPENT < 1000 then uniform(30000,40000,random()) / 100\n        end as TIME_ON_APP,\n        case when YEARLY_SPENT < 300 then uniform(5000,7500,random()) / 100\n            when YEARLY_SPENT < 500 then uniform(7000,15000,random()) / 100\n            when YEARLY_SPENT < 520 then null\n            when YEARLY_SPENT < 1000 then uniform(14000,30000,random()) / 100\n        end as TIME_ON_WEBSITE\nfrom table(generator(rowcount=>100)))\nselect email, gender, MEMBERSHIP_STATUS, MEMBERSHIP_LENGTH, AVG_SESSION_LENGTH, TIME_ON_APP, TIME_ON_WEBSITE, YEARLY_SPENT from profiles;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "153fb7e4-91a7-4e50-bacc-51e29bc5a035",
   "metadata": {
    "language": "python",
    "name": "IMPORTS",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Snowpark Imports\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.metrics.correlation import correlation\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\n\n# Snowflake Task API\nfrom datetime import timedelta\nfrom snowflake.core import Root\nfrom snowflake.core.table import Table\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Other Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84a18675-0de6-443d-8684-9eed6740909a",
   "metadata": {
    "name": "STEP_2_FEATURE_ENGINEERING",
    "collapsed": false
   },
   "source": "# 2 - Feature Engineering"
  },
  {
   "cell_type": "code",
   "id": "ebc1a9d9-13f4-4d25-99a0-22c11b8f8e20",
   "metadata": {
    "language": "python",
    "name": "CREATE_SNOWPARK_SESSION",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get a Session\nsession = get_active_session()\n\n# Create a Snowpark DataFrame\ndf = session.table('ECOMMERCE_CUSTOMERS')\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b86dedc-2e12-41e0-8232-602416b7603b",
   "metadata": {
    "language": "python",
    "name": "DESCRIBE_DATA",
    "collapsed": false
   },
   "outputs": [],
   "source": "df.describe().show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "beeb5984-58d0-482b-9ba7-8b30250e018f",
   "metadata": {
    "language": "python",
    "name": "SPLIT_DATA",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Split the data into train and test sets\ntrain_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\ntrain_df.count(), test_df.count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "702ba321-13fe-40d4-9f17-dd6fef62b5c3",
   "metadata": {
    "language": "python",
    "name": "PREPROCESSING_PIPELINE",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define sklearn-like Imputers and Encoders\nsi_numeric =  SimpleImputer(\n    input_cols=['MEMBERSHIP_LENGTH','AVG_SESSION_LENGTH','TIME_ON_APP','TIME_ON_WEBSITE'], \n    output_cols=['MEMBERSHIP_LENGTH_IMP','AVG_SESSION_LENGTH_IMP','TIME_ON_APP_IMP','TIME_ON_WEBSITE_IMP'],\n    strategy='mean'\n)\n\nsi_categorical = SimpleImputer(\n    input_cols=['MEMBERSHIP_STATUS'], \n    output_cols=['MEMBERSHIP_STATUS_IMP'],\n    strategy='most_frequent'\n)\n\n# Define sklearn-like Encoders\ncategories = {\n    \"MEMBERSHIP_STATUS_IMP\": np.array([\"BASIC\", \"BRONZE\", \"SILVER\", \"GOLD\", \"PLATIN\", \"DIAMOND\"]),\n}\noe_categorical = OrdinalEncoder(\n    input_cols=[\"MEMBERSHIP_STATUS_IMP\"], \n    output_cols=[\"MEMBERSHIP_STATUS_IMP_OE\"], \n    categories=categories\n)\n\nohe_categorical = OneHotEncoder(\n    input_cols=[\"GENDER\"], \n    output_cols=[\"GENDER_OHE\"]\n)\n\n# Build the pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n        (\"SI_NUMERIC\",si_numeric),\n        (\"SI_CATEGORICAL\",si_categorical),\n        (\"OE_CATEGORICAL\",oe_categorical),\n        (\"OHE_CATEGORICAL\",ohe_categorical),\n    ]\n)\n\n# Fit the pipeline and transform data\ntransformed_train_df = preprocessing_pipeline.fit(train_df).transform(train_df)\ntransformed_train_df.show()\n\ntransformed_test_df = preprocessing_pipeline.transform(test_df)\ntransformed_test_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be3d6703-addc-4570-913d-6e6240a36c89",
   "metadata": {
    "language": "python",
    "name": "DROP_UNUSED_COLS",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Drop unused columns\ntransformed_train_df = transformed_train_df.drop([\n    'GENDER',\n    'MEMBERSHIP_STATUS',\n    'MEMBERSHIP_LENGTH',\n    'AVG_SESSION_LENGTH',\n    'TIME_ON_APP',\n    'TIME_ON_WEBSITE']\n)\ntransformed_test_df = transformed_test_df.drop([\n    'GENDER',\n    'MEMBERSHIP_STATUS',\n    'MEMBERSHIP_LENGTH',\n    'AVG_SESSION_LENGTH',\n    'TIME_ON_APP',\n    'TIME_ON_WEBSITE']\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09a13117-d8f4-4826-a386-5bd343201f21",
   "metadata": {
    "language": "python",
    "name": "CORRELATION",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Calculate correlations\ncorr_train_df = correlation(df=transformed_train_df)\n#corr_train_df # This is a Pandas DataFrame\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_train_df, dtype=bool))\n\n# Create a heatmap with the features\nplt.figure(figsize=(10, 5))\nheatmap = sns.heatmap(corr_train_df.round(3), mask=mask, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f54de6ab-3611-470b-bea7-64708c33394c",
   "metadata": {
    "language": "sql",
    "name": "INCREASE_COMPUTE",
    "collapsed": false
   },
   "outputs": [],
   "source": "ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE='MEDIUM';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4ba89dc-2d0a-46a3-bbf7-252b7ad272e3",
   "metadata": {
    "name": "STEP_3_MODEL_TRAINING",
    "collapsed": false
   },
   "source": "# 3 - Model Training"
  },
  {
   "cell_type": "code",
   "id": "1a426672-6327-45fa-99c0-af798785e845",
   "metadata": {
    "language": "python",
    "name": "PARAMETER_TUNING",
    "collapsed": false
   },
   "outputs": [],
   "source": "feature_cols = [\n    'GENDER_OHE_FEMALE',\n    'GENDER_OHE_MALE',\n    'MEMBERSHIP_STATUS_IMP_OE',\n    'MEMBERSHIP_LENGTH_IMP',\n    'AVG_SESSION_LENGTH_IMP',\n    'TIME_ON_APP_IMP',\n    'TIME_ON_WEBSITE_IMP'\n]\nlabel_cols = ['YEARLY_SPENT']\noutput_cols = ['YEARLY_SPENT_PREDICTION']\n\n\ngrid_search = GridSearchCV(\n    estimator=XGBRegressor(),\n    param_grid={\n        \"n_estimators\":[100, 200],\n        \"learning_rate\":[0.1, 0.2],\n    },\n    n_jobs = -1,\n    scoring=\"neg_mean_absolute_percentage_error\",\n    input_cols=feature_cols,\n    label_cols=label_cols,\n    output_cols=output_cols\n)\n\n# Train\ngrid_search.fit(transformed_train_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "382d47af-fcd3-4d98-b1a7-38b4cd88b723",
   "metadata": {
    "language": "sql",
    "name": "DECREASE_COMPUTE"
   },
   "outputs": [],
   "source": "ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE='XSMALL';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "928dfeb8-f4dd-42aa-a336-5cc40a8713d6",
   "metadata": {
    "name": "STEP_4_MODEL_EVALUATION",
    "collapsed": false
   },
   "source": "# 4 - Model Evaluation"
  },
  {
   "cell_type": "code",
   "id": "85cdccdb-c7a1-46f1-a4b4-5cf643f95e51",
   "metadata": {
    "language": "python",
    "name": "ANALYZE_GRIDSEARCH",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Analyze grid search results\ngs_results = grid_search.to_sklearn().cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b11b16fe-f967-4763-b958-1b52e393b339",
   "metadata": {
    "language": "python",
    "name": "EVALUATE_MODEL",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Predict\nresult = grid_search.predict(transformed_test_df)\n\n# Analyze results\nmape = mean_absolute_percentage_error(df=result, \n                                        y_true_col_names=\"YEARLY_SPENT\", \n                                        y_pred_col_names=\"YEARLY_SPENT_PREDICTION\")\n\nresult.select(\"YEARLY_SPENT\", \"YEARLY_SPENT_PREDICTION\").show()\nprint(f\"Mean absolute percentage error: {mape}\")\n\n# Plot actual vs predicted \ng = sns.relplot(\n    data=result[\"YEARLY_SPENT\", \"YEARLY_SPENT_PREDICTION\"].to_pandas().astype(\"float64\"), \n    x=\"YEARLY_SPENT\", \n    y=\"YEARLY_SPENT_PREDICTION\", \n    kind=\"scatter\")\ng.ax.axline((0,0), slope=1, color=\"r\")\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fc448e4-93e0-45b8-9091-606986388460",
   "metadata": {
    "name": "STEP_5_REGISTER_MODEL",
    "collapsed": false
   },
   "source": "# 5 - Register & Run Model"
  },
  {
   "cell_type": "code",
   "id": "a6cc7db6-9876-4ac3-9e5f-c62d4b7cb8fb",
   "metadata": {
    "language": "python",
    "name": "REGISTER_RUN_MODEL",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create reference to model registry\nreg = Registry(session=session, database_name=\"MACHINE_LEARNING\", schema_name=\"PUBLIC\")\n\n# Get latest model version\ntry:\n    model_versions = reg.get_model(\"ECOMMERCE_SPENT_MODEL\").show_versions()\n    idx = model_versions['created_on'].idxmax()\n    most_recent_version = model_versions.loc[idx]\n    new_version = 'V'+str(int(most_recent_version['name'][1:])+1)\nexcept:\n    new_version = 'V0'\n\n# Register new model version\nregistered_model = reg.log_model(\n    grid_search,\n    model_name=\"ECOMMERCE_SPENT_MODEL\",\n    version_name=new_version,\n    comment=\"Model trained using GridsearchCV in Snowpark to predict customer's yearly spending.\",\n    metrics={\"mean_abs_pct_err\": mape},\n    sample_input_data=transformed_train_df.select(feature_cols).limit(100)\n)\n\n# Create predictions from registered model\npredictions_df = registered_model.run(transformed_test_df, function_name=\"predict\")\npredictions_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f94190b-6287-42de-92d6-dc1f81b802b9",
   "metadata": {
    "language": "python",
    "name": "CREATE_TMP_VIEW",
    "collapsed": false
   },
   "outputs": [],
   "source": "transformed_test_df.create_or_replace_temp_view('INFERENCE_TEST')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "150cd7e2-41a0-4800-9a8c-323de6c6fada",
   "metadata": {
    "language": "sql",
    "name": "RUN_MODEL_SQL",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT *, \n    ECOMMERCE_SPENT_MODEL!PREDICT(\n        \"GENDER_OHE_FEMALE\",\n        \"GENDER_OHE_MALE\",\n        \"MEMBERSHIP_STATUS_IMP_OE\",\n        \"MEMBERSHIP_LENGTH_IMP\",\n        \"AVG_SESSION_LENGTH_IMP\",\n        \"TIME_ON_APP_IMP\",\n        \"TIME_ON_WEBSITE_IMP\")['YEARLY_SPENT_PREDICTION'] AS YEARLY_SPENT_PREDICTION\nFROM INFERENCE_TEST;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5aaa594-b5f1-49e4-b37c-fce99f459e27",
   "metadata": {
    "name": "STEP_6_AUTOMATE",
    "collapsed": false
   },
   "source": "# 6 - Automate Pipeline"
  },
  {
   "cell_type": "code",
   "id": "969c3e3b-9339-49db-90e7-131f3f81f575",
   "metadata": {
    "language": "sql",
    "name": "CREATE_DAG_STAGE",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Create a stage to store Pipeline artifacts\nCREATE OR REPLACE STAGE DAG_STAGE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69f38566-a191-414a-aaad-a4d911548a70",
   "metadata": {
    "language": "python",
    "name": "DEFINE_DAG_FUNCTIONS",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define preprocessing task\ndef preprocess_data(session: Session) -> str:\n    df = session.table('ECOMMERCE_CUSTOMERS')\n    transformed_df = preprocessing_pipeline.transform(df)\n    transformed_df.write.save_as_table('ECOMMERCE_CUSTOMERS_PREPARED', mode='overwrite')\n    num_rows = session.table('ECOMMERCE_CUSTOMERS_PREPARED').count()\n    return f\"Prepared new training table with {num_rows} customers.\"\n\n# Define model training task\ndef train_model(session: Session) -> str:\n    reg = Registry(session=session, database_name=\"MACHINE_LEARNING\", schema_name=\"PUBLIC\")\n    df = session.table('ECOMMERCE_CUSTOMERS_PREPARED')\n    feature_cols = [\n        'GENDER_OHE_FEMALE',\n        'GENDER_OHE_MALE',\n        'MEMBERSHIP_STATUS_IMP_OE',\n        'MEMBERSHIP_LENGTH_IMP',\n        'AVG_SESSION_LENGTH_IMP',\n        'TIME_ON_APP_IMP',\n        'TIME_ON_WEBSITE_IMP'\n    ]\n    label_cols = ['YEARLY_SPENT']\n    output_cols = ['YEARLY_SPENT_PREDICTION']\n\n    # Define parameter tuning\n    grid_search = GridSearchCV(\n        estimator=XGBRegressor(),\n        param_grid={\n            \"n_estimators\":[100, 200],\n            \"learning_rate\":[0.1, 0.2],\n        },\n        n_jobs = -1,\n        scoring=\"neg_mean_absolute_percentage_error\",\n        input_cols=feature_cols,\n        label_cols=label_cols,\n        output_cols=output_cols\n    )\n    \n    # Train\n    grid_search.fit(df)\n\n    # Get latest model version\n    model_versions = reg.get_model(\"ECOMMERCE_SPENT_MODEL\").show_versions()\n    idx = model_versions['created_on'].idxmax()\n    most_recent_version = model_versions.loc[idx]\n    new_version = 'V'+str(int(most_recent_version['name'][1:])+1)\n\n    # Register new model version\n    registered_model = reg.log_model(\n        grid_search,\n        model_name=\"ECOMMERCE_SPENT_MODEL\",\n        version_name=new_version,\n        comment=\"Model trained using GridsearchCV in Snowpark to predict customer's yearly spending.\",\n        sample_input_data=df.select(feature_cols).limit(100)\n    )\n    \n    return f\"Registered new model with version: {new_version}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7305e755-dc46-43ba-819a-6813a0267e08",
   "metadata": {
    "language": "python",
    "name": "DEFINE_AND_DEPLOY_DAG",
    "collapsed": false
   },
   "outputs": [],
   "source": "root = Root(session)\n\nwith DAG(\"MY_TRAINING_PIPELINE\", stage_location='DAG_STAGE', schedule=timedelta(minutes=10)) as dag:\n    task_preprocess_data = DAGTask(\n        \"PREPROCESS_DATA\",\n        definition=StoredProcedureCall(preprocess_data, stage_location='DAG_STAGE', packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\"]),\n        warehouse=\"COMPUTE_WH\"\n    )\n    task_train_model = DAGTask(\n        \"TRAIN_MODEL\",\n        definition=StoredProcedureCall(train_model, stage_location='DAG_STAGE', packages=[\"snowflake-snowpark-python\",\"snowflake-ml-python\"]),\n        warehouse=\"COMPUTE_WH\"\n    )\n\ntask_preprocess_data >> task_train_model\nschema = root.databases[\"MACHINE_LEARNING\"].schemas[\"PUBLIC\"]\ndag_op = DAGOperation(schema)\ndag_op.deploy(dag, mode=CreateMode.or_replace)\ndag_op.run(dag)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2937a21b-1873-42a0-8abb-ebe441c2cf66",
   "metadata": {
    "name": "STEP_7_CLEANUP",
    "collapsed": false
   },
   "source": "# 7 - Clean Up"
  },
  {
   "cell_type": "code",
   "id": "baa8fedc-4eb8-4d82-9719-188ed5db029b",
   "metadata": {
    "language": "python",
    "name": "CLEANUP",
    "collapsed": false
   },
   "outputs": [],
   "source": "*STOP*\n\n# Delete Tasks\nroot = Root(session)\ntasks = root.databases[\"MACHINE_LEARNING\"].schemas[\"PUBLIC\"].tasks\n\ntask_res = tasks['MY_TRAINING_PIPELINE']\ntask_res.suspend()\ntask_res.delete()\n\ntask_res = tasks['MY_TRAINING_PIPELINE$PREPROCESS_DATA']\ntask_res.suspend()\ntask_res.delete()\n\ntask_res = tasks['MY_TRAINING_PIPELINE$TRAIN_MODEL']\ntask_res.suspend()\ntask_res.delete()\n\nfor t in root.databases[\"MACHINE_LEARNING\"].schemas[\"PUBLIC\"].tasks.iter(like='%'):\n    print(t.name)\n\n# Delete Data\nmy_table_res = root.databases[\"MACHINE_LEARNING\"].schemas[\"PUBLIC\"].tables[\"ECOMMERCE_CUSTOMERS_PREPARED\"]\nmy_table_res.delete()\n\nmy_table_res = root.databases[\"MACHINE_LEARNING\"].schemas[\"PUBLIC\"].tables[\"ECOMMERCE_CUSTOMERS\"]\nmy_table_res.delete()\n\n# Delete models\nreg = Registry(session=session, database_name=\"MACHINE_LEARNING\", schema_name=\"PUBLIC\")\nreg.delete_model(\"ECOMMERCE_SPENT_MODEL\")\n\n# Drop stage\nsession.sql('DROP STAGE DAG_STAGE').collect()",
   "execution_count": null
  }
 ]
}