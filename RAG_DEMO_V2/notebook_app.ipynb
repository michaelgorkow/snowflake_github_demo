{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0279aa7-46d6-4386-be33-046ddda6792e",
   "metadata": {
    "name": "TITLE",
    "collapsed": false
   },
   "source": "# RAG Demo in Snowflake\n\n1. Extract Text from PDF via PyPDF2 in Python\n2. Split texts into chunks using langchain in Python\n3. Convert text-chunks into embeddings using Snowflake Cortex\n4. Use Vector-Search and LLM for Retrievel Augmented Generation"
  },
  {
   "cell_type": "code",
   "id": "1cab556c-120d-4a9d-baa3-9fa269ac9174",
   "metadata": {
    "language": "python",
    "name": "IMPORTS",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nfrom snowflake.snowpark.functions import udtf, udf\nfrom snowflake.snowpark.files import SnowflakeFile\nfrom io import BytesIO\nfrom typing import Iterable, Tuple\n\n# Get Snowpark Session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6abf2285-14a3-40c5-b7fa-51fed66875a2",
   "metadata": {
    "name": "start_text_extraction",
    "collapsed": false
   },
   "source": "### 1. Python PDF Extraction\n\nCreating a Python User-Defined-Table-Function (UDTF) using PyPDF2 to extract text from PDF files."
  },
  {
   "cell_type": "code",
   "id": "edf8910c-848e-46a0-ac66-41d82251f487",
   "metadata": {
    "language": "sql",
    "name": "VIEW_DOCUMENTS",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DIRECTORY('@DOCUMENTS');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3725ce6-4e3c-4506-9547-83543fcfc922",
   "metadata": {
    "language": "python",
    "name": "udf_pdf_extraction",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "@udtf(session=session,\n      name='READ_PDF_UDTF',\n      replace=True,\n      is_permanent=True,\n      stage_location='@GENAI.PUBLIC.FUNCTIONS',\n      packages=['snowflake-snowpark-python','pypdf2','pycryptodome'], \n      output_schema=['PAGE_NUM','TEXT'])\nclass udtf_pdf_reader:\n    def process(self, file_path:str) -> Iterable[Tuple[int,str]]:\n        from PyPDF2 import PdfFileReader\n        with SnowflakeFile.open(file_path, 'rb') as file:\n            f = BytesIO(file.readall())\n            pdf_reader = PdfFileReader(f)\n            for page_ix, page in enumerate(pdf_reader.pages):\n                yield (page_ix, page.extract_text())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "539565a1-aadb-440c-ac3e-42d9f0ab40fc",
   "metadata": {
    "language": "sql",
    "name": "EXTRACT_TEXT",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- We could use Python for PDF extraction but every SQL analyst can use the function as well\nCREATE OR REPLACE TABLE RAW_TEXT AS\nSELECT relative_path,\n       file_url,\n       func.*\n    FROM directory(@DOCUMENTS),\n        TABLE(READ_PDF_UDTF(build_scoped_file_url(@DOCUMENTS, relative_path))) as func;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7064de84-3b2b-4447-89c2-bb93d4b653f2",
   "metadata": {
    "language": "python",
    "name": "CELL2"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.session import Session\nfrom snowflake.snowpark.functions import col, regexp_replace\n\n# Assuming you've already established a session with Snowflake\n# session = Session.builder.configs(configs).create()\n\n# Assuming there's a DataFrame `df` with a column named 'text_column' that you want to process\n# For example:\n# df = session.create_dataframe([\"This is   an example    text with  spaces\"])\n\n# Use regexp_replace to find whitespaces that occur more than twice in a row and replace them\n# Here, the pattern '\\\\s{2,}' means \"match any whitespace character that occurs at least twice in a row\"\n# and we replace it with a single space ' '\ndf_modified = df.with_column(\"text_column\", regexp_replace(col(\"text_column\"), '\\\\s{2,}', ' '))\n\n# Show the modified DataFrame\ndf_modified.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81a4095c-021a-43ec-bbcf-a05d856359c7",
   "metadata": {
    "language": "python",
    "name": "snowpark_dataframe",
    "collapsed": false
   },
   "outputs": [],
   "source": "# And we can easily switch between SQL and Python\npdf_extracts = session.table('RAW_TEXT')\nprint('Number of pages extracted:', pdf_extracts.count())\npdf_extracts.limit(5).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f0cd6aa-e402-45b2-9029-72bc19887aca",
   "metadata": {
    "name": "start_text_splitting",
    "collapsed": false
   },
   "source": "## 2. Text Splitting with Langchain\nSome pages contain a lot of text, so we want to split them into chunks that fit into our embedding model.\n\nWe use another UDTF based on langchain for this."
  },
  {
   "cell_type": "code",
   "id": "7a8e110b-b674-46e6-8a0a-41523806da20",
   "metadata": {
    "language": "python",
    "name": "langchain_splitting",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "@udtf(session=session,\n      name='langchain_splitting',\n      replace=True,\n      is_permanent=True,\n      stage_location='@GENAI.PUBLIC.FUNCTIONS',\n      packages=['langchain'], \n      output_schema=['CHUNK_INDEX','TEXT_CHUNK'])\nclass udtf_text_splitter:\n    def __init__(self):\n        from langchain.text_splitter import RecursiveCharacterTextSplitter\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            separators = [\"\\n\"],\n            chunk_size = 1000,\n            chunk_overlap  = 50,\n            length_function = len,\n            is_separator_regex = False\n        )\n    def process(self, text:str) -> Iterable[Tuple[int,str]]:\n        texts = self.text_splitter.create_documents([text])\n        for chunk_ix, text in enumerate(texts):\n            yield (chunk_ix, text.page_content.strip())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcc9557d-6a7f-41f6-aee5-051a0ca35a66",
   "metadata": {
    "language": "python",
    "name": "langchain_splitting2",
    "collapsed": false
   },
   "outputs": [],
   "source": "pdf_extracts_chunked = pdf_extracts.join_table_function('langchain_splitting', 'text')\npdf_extracts_chunked = pdf_extracts_chunked.drop('TEXT')\npdf_extracts_chunked.write.save_as_table('CHUNK_TEXT', mode='overwrite')\nsession.table('CHUNK_TEXT').order_by('PAGE_NUM','CHUNK_INDEX').limit(10).to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ad7d0b0-0b31-48df-8c27-cc38b5f70917",
   "metadata": {
    "name": "start_text_embedding",
    "collapsed": false
   },
   "source": "### 3. Convert Text-Chunks to Text-Embddings using Snowflake Cortex Functions"
  },
  {
   "cell_type": "code",
   "id": "84530384-5bdc-4db3-b458-3796dd215841",
   "metadata": {
    "language": "sql",
    "name": "cortex_text_embedding",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Convert your chunks to embeddings\nCREATE OR REPLACE TABLE VECTOR_STORE AS\n    SELECT RELATIVE_PATH,\n           PAGE_NUM,\n           TEXT_CHUNK,\n           snowflake.cortex.embed_text('e5-base-v2', TEXT_CHUNK) as chunk_embedding\n        FROM CHUNK_TEXT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35e90c39-9afe-4876-a7fd-d81f4073d351",
   "metadata": {
    "language": "sql",
    "name": "embedding_search",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Embedding Search\nSELECT PAGE_NUM, \n       TEXT_CHUNK,\n       VECTOR_L2_DISTANCE(\n            snowflake.cortex.embed_text('e5-base-v2', \n                'What is Siemens Smart Infrastructure?'), \n            CHUNK_EMBEDDING) AS VECTOR_DISTANCE\nFROM VECTOR_STORE \n    ORDER BY VECTOR_DISTANCE LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ea29c53-3f7e-44b2-8290-7975e580d29a",
   "metadata": {
    "name": "start_rag",
    "collapsed": false
   },
   "source": "### 4. Retrieval Augmented Generation (RAG)"
  },
  {
   "cell_type": "code",
   "id": "a21ab082-2c59-4269-aa8a-ad25ebc53a01",
   "metadata": {
    "language": "sql",
    "name": "sql_rag",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- RAG\nWITH RAG_DATA AS (\n    -- Embedding Search\n    SELECT PAGE_NUM, \n           TEXT_CHUNK,\n           VECTOR_L2_DISTANCE(\n                snowflake.cortex.embed_text('e5-base-v2', \n                    'What is Siemens Smart Infrastructure?'), \n                CHUNK_EMBEDDING) AS VECTOR_DISTANCE\n    FROM VECTOR_STORE \n        ORDER BY VECTOR_DISTANCE LIMIT 1\n)\nSELECT PAGE_NUM, \n       TEXT_CHUNK,\n       snowflake.cortex.complete(\n            'mixtral-8x7b', \n            CONCAT( \n                'Answer the question based on the context. Be concise.','Context: ',\n                TEXT_CHUNK,\n                'Question: ', \n                'What is Siemens Smart Infrastructure?',\n                'Answer: '\n            )\n        ) as RESPONSE\n    FROM RAG_DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b0ca27e-9374-4155-8f83-c3f6a4bf29b6",
   "metadata": {
    "language": "sql",
    "name": "sql_rag2_system_prompts",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- RAG\nWITH RAG_DATA AS (\n    -- Embedding Search\n    SELECT PAGE_NUM, \n           TEXT_CHUNK,\n           VECTOR_L2_DISTANCE(\n                snowflake.cortex.embed_text('e5-base-v2', \n                    'What is Siemens Smart Infrastructure?'), \n                CHUNK_EMBEDDING) AS VECTOR_DISTANCE\n    FROM VECTOR_STORE \n        ORDER BY VECTOR_DISTANCE LIMIT 1\n)\nSELECT PAGE_NUM, \n       TEXT_CHUNK,\n       snowflake.cortex.complete(\n            'mixtral-8x7b', \n            [\n                {'role': 'system', 'content': 'You are a helpful AI assistant that answers questions based on provided context. Be concise.'},\n                {'role': 'user', 'content': CONCAT('What is Siemens Smart Infrastructure? The context is:', TEXT_CHUNK)}\n            ], {}\n        ) as RESPONSE\n    FROM RAG_DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3d9f513-d94d-440d-89b9-819c09af13df",
   "metadata": {
    "language": "python",
    "name": "STREAMLIT",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\n\n# Write directly to the app\nst.title(\"Snowflake Cortex\")\nst.subheader(\"Retrieval Augmented Generation for Siemens Financial Report\")\n\n# Get the current credentials\nsession = get_active_session()\n\nllm_model = st.selectbox('LLM Model', ['mistral-large', 'mixtral-8x7b', 'llama2-70b-chat', 'mistral-7b', 'gemma-7b'])\n\nquestion = st.text_area('Your question:', value='What is Siemens Smart Infrastructure?')\nif st.button('Ask question'):\n    sql = f\"\"\"WITH RAG_DATA AS (\n                    -- Embedding Search\n                    SELECT PAGE_NUM, \n                           TEXT_CHUNK,\n                           VECTOR_L2_DISTANCE(\n                                snowflake.cortex.embed_text('e5-base-v2', \n                                    '{question}'), \n                                CHUNK_EMBEDDING) AS VECTOR_DISTANCE\n                    FROM VECTOR_STORE \n                        ORDER BY VECTOR_DISTANCE LIMIT 1\n                )\n                SELECT PAGE_NUM, \n                       TEXT_CHUNK,\n                       snowflake.cortex.complete(\n                            '{llm_model}', \n                            CONCAT( \n                                'Answer the question based on the context. Be concise.','Context: ',\n                                TEXT_CHUNK,\n                                'Question: ', \n                                '{question}',\n                                'Answer: '\n                            )\n                        ) as RESPONSE\n                    FROM RAG_DATA\"\"\"\n    results = session.sql(sql).collect()[0]\n    st.subheader('Response:')\n    st.info(results['RESPONSE'])\n    st.subheader('Context:')\n    st.info(f\"Page Number: {results['PAGE_NUM']}\")\n    st.info(f\"Page Context: {results['TEXT_CHUNK']}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d67979b-c390-4d5e-a8b1-966de5f8bd7b",
   "metadata": {
    "language": "python",
    "name": "CLEANUP"
   },
   "outputs": [],
   "source": "# clean up\n#session.sql('DROP TABLE IF EXISTS CHUNK_TEXT').collect()\n#session.sql('DROP TABLE IF EXISTS RAW_TEXT').collect()\n#session.sql('DROP TABLE IF EXISTS VECTOR_STORE').collect()\n#session.sql('DROP FUNCTION IF EXISTS READ_PDF_UDTF(VARCHAR)').collect()\n#session.sql('DROP FUNCTION IF EXISTS LANGCHAIN_SPLITTING(VARCHAR)').collect()",
   "execution_count": null
  }
 ]
}